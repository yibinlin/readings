<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>misc on Yibin Lin's Reading Blog</title><link>https://yibinlin.github.io/readings/tags/misc/</link><description>Recent content in misc on Yibin Lin's Reading Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 10 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://yibinlin.github.io/readings/tags/misc/index.xml" rel="self" type="application/rss+xml"/><item><title>Diffusion Model Study Notes</title><link>https://yibinlin.github.io/readings/posts/diffusion_model_study/</link><pubDate>Sat, 10 Feb 2024 00:00:00 +0000</pubDate><guid>https://yibinlin.github.io/readings/posts/diffusion_model_study/</guid><description>Basic modules of Stable Diffusion Model Basic tutorial: https://comfyanonymous.github.io/ComfyUI_tutorial_vn/ CLIP: encode text, another name: text encoder Sampler: takes the main Stable Diffusion model as an input, takes both positive and negative prompts encoded by CLIP model + a latent image (can be blank) sampler takes this input latent image, adds noise to it and then denoises it using the main model prompts and negative prompts are passed to model at each sampling step sampler outputs the denoised image VAE: translates an image from latent space to pixel space Prompting: (word:1.</description></item></channel></rss>